# ai_mode_module.py

import re
import pandas as pd
from collections import defaultdict
from difflib import get_close_matches
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# ì „ì—­ ë³€ìˆ˜ë“¤
vectorizer = None
df_model = None
preprocessor = None
required_cols = ['L1 NAME', 'L2 NAME', 'L3 NAME', 'L4 NAME']
hierarchical_models = {}
hierarchical_label_encoders = {}

# âœ… SimplePreprocessor í´ë˜ìŠ¤ ì •ì˜
class SimplePreprocessor:
    def __init__(self, df_model: pd.DataFrame):
        self.df_model = df_model
        self.stop_words = set(["OF", "FOR", "WITH", "AND", "THE", "A", "AN", "TO", "IN", "ON", "BY"])
        self.unit_dict = self._learn_units()
        self.synonyms = self._learn_synonyms()
        self.manual_synonyms = {
    "SET": ["SET", "SETS", "KIT", "KITS", "KITAGAWA", "KITGRACO", "LIGHTINGKIT"],
    "BOX": ["BOX", "BOXES", "BOXING", "CASE", "CASES", "CARTON"],
    "BOTTLE": ["BTL", "BTLS", "BOTTLE", "BOTTLES", "FLASK"],
    "SPOON": ["SPOON", "SPOONS", "TEASPOON", "TABLESPOON", "LADLE"],
    "GLASS": ["GLASS", "GLASSES", "GLASSWARE", "FIBERGLASS", "FIBREGLASS", "MARINEGLASS"],
    "JAR": ["JAR", "JARS", "JARLSBERG", "CANISTER", "CONTAINER"],
    "MUG": ["MUG", "MUGS", "CUP", "CUPS", "TUMBLER", "STEIN"],
    "TRAY": ["TRAY", "TRAYS", "PLATE", "PLATES", "DISH", "DISHES"],
    "TAPE": ["TAPE", "TAPES", "BAND", "STRAP", "STRAPS"],
    "HANDLE": ["HANDLE", "KNOB", "GRIP", "KNOBSET", "LEVER"],
    "VALVE": ["VALVE", "VALVES", "COCK", "STOPPER", "TAP"],
    "PIPE": ["PIPE", "TUBE", "TUBES", "HOSE", "DUCT", "CYLINDER"],
    "BLADE": ["BLADE", "BLADES", "KNIFE", "KNIVES", "CUTTER", "CUTTERS"],
    "COVER": ["COVER", "COVERS", "LID", "CAP", "SLEEVE", "SHIELD"],
    "CLOTH": ["CLOTH", "CLOTHES", "FABRIC", "TEXTILE", "RAG", "RAGS"],
    "GLOVE": ["GLOVE", "GLOVES", "MITT", "MITTS", "HANDGUARD"],
    "WIRE": ["WIRE", "CABLE", "CORD", "ROPE", "LINE"],
    "HELMET": ["HELMET", "CAP", "HEADGEAR", "HAT", "HARDHAT"],
    "GOGGLES": ["GOGGLES", "GLASSES", "SHADES", "VISOR", "PROTECTIVE EYEWEAR"],
    "BATTERY": ["BATTERY", "BATTERIES", "CELL", "ACCUMULATOR"],
    "PAINT": ["PAINT", "COATING", "SPRAY", "VARNISH", "ENAMEL"],
    "BAG": ["BAG", "BAGS", "SACK", "SACKS", "POUCH", "PACK"],
    "LABEL": ["LABEL", "TAG", "STICKER", "DECAL"],
    "CLEANER": ["CLEANER", "DETERGENT", "SOAP", "SANITIZER", "DISINFECTANT"],
    "FAN": ["FAN", "FANS", "VENTILATOR", "BLOWER"],
    "LIGHT": ["LIGHT", "LAMP", "LED", "BULB", "FIXTURE"],
    "PUMP": ["PUMP", "PUMPS", "DISPENSER", "INJECTOR"],
        }
        self.tfidf_model = self._learn_tfidf_model()

    def _learn_units(self):
        units = defaultdict(int)
        specs = self.df_model['L5 NAME (SPEC)'].dropna().astype(str)
        for spec in specs:
            for match in re.findall(r'(\d+)\s*([A-Z]{1,5})', spec.upper()):
                units[match[1]] += 1
        return dict(units)

    def _learn_synonyms(self):
        synonyms = {}
        for col in ['L1 NAME', 'L2 NAME', 'L3 NAME', 'L4 NAME']:
            if col in self.df_model.columns:
                for val in self.df_model[col].dropna().unique():
                    if '/' in val:
                        terms = [t.strip().upper() for t in val.split('/')]
                        for t in terms:
                            synonyms[t] = terms
        return synonyms

    def _learn_tfidf_model(self):
        texts = self.df_model[['L1 NAME', 'L2 NAME', 'L3 NAME', 'L4 NAME']].fillna('').agg(' '.join, axis=1)
        vectorizer = TfidfVectorizer()
        tfidf_model_matrix = vectorizer.fit_transform(texts)
        return dict(zip(vectorizer.get_feature_names_out(), tfidf_model_matrix.sum(axis=0).A1))

    def normalize_query(self, query: str):
        query = query.upper()
        query = re.sub(r"[^A-Z0-9\s/]", " ", query)
        for unit in self.unit_dict:
            query = re.sub(rf"(\d+)\s*{unit}", rf"\1{unit}", query)
        return query.strip()

    def expand_query(self, query: str):
        words = set()
        for token in query.split():
            if token in self.stop_words:
                continue
            words.add(token)
            if token in self.synonyms:
                words.update(self.synonyms[token])
            if token in self.manual_synonyms:
                words.update(self.manual_synonyms[token])
            if '/' in token:
                parts = token.split('/')
                words.update(parts)
                words.add(' '.join(parts))
        return ' '.join(sorted(words))

    def score_query(self, query: str):
        words = query.split()
        return sum(self.tfidf_model.get(w.lower(), 0) for w in words)

    def preprocess(self, query: str):
        norm = self.normalize_query(query)
        expanded = self.expand_query(norm)
        score = self.score_query(expanded)
        return {
            'original': query,
            'normalized': norm,
            'expanded': expanded,
            'tfidf_model_score': round(score, 4)
        }

    def preprocess_normalized(self, query: str):
        return self.normalize_query(query)

# ğŸ” ì˜ˆì¸¡ í•¨ìˆ˜
def predict_item_name(raw_name, verbose=True):
    raw_name = raw_name.upper()
    normalized = preprocessor.preprocess_normalized(raw_name)

    context = normalized
    prediction = {}

    for col in required_cols:
        model, vec = hierarchical_models[col]
        input_vec = vec.transform([context])
        label_id = model.predict(input_vec)[0]
        label = hierarchical_label_encoders[col].inverse_transform([label_id])[0]
        prediction[col] = label
        context = f"{context} {label}"

    input_vec = vectorizer.transform([normalized])
    candidate_vecs = vectorizer.transform(df_model["ITEM_NAME"])
    similarities = cosine_similarity(input_vec, candidate_vecs).flatten()

    top_idx = similarities.argmax()
    predicted_l5 = df_model.iloc[top_idx]["L5 NAME (SPEC)"]
    prediction["L5 NAME (SPEC)"] = predicted_l5

    if verbose:
        print(f"ğŸ” L5ëŠ” ìœ ì‚¬ë„ ê¸°ë°˜ ì¶”ì²œë¨ (ìœ ì‚¬ë„: {round(similarities[top_idx], 3)})")
    return prediction

from sklearn.metrics.pairwise import cosine_similarity
from difflib import get_close_matches
import numpy as np
import pandas as pd

# ğŸ”§ ì˜¤íƒ€ ë³´ì • + í›„ë³´ ì„ íƒ + ìœ ì‚¬ë„ ì ìˆ˜ í‘œì‹œ
def autocorrect_input(user_input, top_n=3, cutoff=0.6):
    input_upper = user_input.upper()
    candidates = df_model["ITEM_NAME"].unique()

    # 1ì°¨ í›„ë³´: ë¬¸ì ê¸°ë°˜ ìœ ì‚¬ í’ˆëª…
    close = get_close_matches(input_upper, candidates, n=top_n, cutoff=cutoff)
    if not close:
        return input_upper  # í›„ë³´ ì—†ìŒ â†’ ì›ë³¸ ìœ ì§€

    # TF-Idf_model ìœ ì‚¬ë„ ê³„ì‚°
    preprocessed_input = preprocessor.preprocess(input_upper)["normalized"]
    input_vec = vectorizer.transform([preprocessed_input])
    candidate_vecs = vectorizer.transform(close)
    similarities = cosine_similarity(input_vec, candidate_vecs).flatten()

    # í›„ë³´ ë¦¬ìŠ¤íŠ¸ ì •ë¦¬
    scored_candidates = list(zip(close, similarities))
    scored_candidates.sort(key=lambda x: x[1], reverse=True)

    print("\nğŸ” ì…ë ¥í•˜ì‹  í’ˆëª…ê³¼ ìœ ì‚¬í•œ í›„ë³´ë¥¼ ì°¾ì•˜ì–´ìš”:")
    for i, (text, score) in enumerate(scored_candidates, 1):
        print(f"  {i}. {text}  (ìœ ì‚¬ë„: {round(score, 3)})")

    # ì‚¬ìš©ì ì„ íƒ
    try:
        selection = int(input("ğŸ‘‰ ì–´ë–¤ í›„ë³´ê°€ ë§ë‚˜ìš”? (ë²ˆí˜¸ ì…ë ¥, 0 = ì›ë˜ ì…ë ¥ ìœ ì§€): "))
        if selection in range(1, len(scored_candidates) + 1):
            return scored_candidates[selection - 1][0]
        else:
            return input_upper
    except ValueError:
        return input_upper


# ğŸ” ìœ ì‚¬ í’ˆëª© ì¶”ì²œ
def recommend_similar_items(input_name, top_n=5):
    preprocessed = preprocessor.preprocess(input_name.upper())["normalized"]
    input_vec = vectorizer.transform([preprocessed])

    # L1~L4 êµ¬ì¡°ê°€ ë™ì¼í•œ ê²ƒë§Œ ë¹„êµ ëŒ€ìƒìœ¼ë¡œ ì œí•œ
    predicted = predict_item_name(input_name)
    l1 = predicted["L1 NAME"]
    l2 = predicted["L2 NAME"]
    l3 = predicted["L3 NAME"]
    l4 = predicted["L4 NAME"]

    # í›„ë³´ í•„í„°ë§
    filtered_df_model = df_model[
        (df_model["L1 NAME"] == l1) &
        (df_model["L2 NAME"] == l2) &
        (df_model["L3 NAME"] == l3) &
        (df_model["L4 NAME"] == l4)
    ]

    if filtered_df_model.empty:
        return []

    candidate_vecs = vectorizer.transform(filtered_df_model["SEARCH_NAME"])
    similarities = cosine_similarity(input_vec, candidate_vecs).flatten()

    top_indices = similarities.argsort()[::-1]
    recommendations = []

    for idx in top_indices:
        if similarities[idx] <= 0:
            continue

        row = filtered_df_model.iloc[idx]
        rec = {
            "SIMILAR_ITEM_NAME": row["ITEM_NAME"],
            "SIMILARITY_SCORE": round(similarities[idx], 3),
            "L1 NAME": row["L1 NAME"],
            "L2 NAME": row["L2 NAME"],
            "L3 NAME": row["L3 NAME"],
            "L4 NAME": row["L4 NAME"],
            "L5 NAME (SPEC)": row["L5 NAME (SPEC)"],
            "P CODE": row["P CODE"]
        }
        recommendations.append(rec)
        if len(recommendations) >= top_n:
            break

    return recommendations


def generate_structured_pcode_based_on_similar(similar_items, fallback_predicted):
    if not similar_items:
        return generate_structured_pcode(fallback_predicted)
    
    # ê°€ì¥ ìœ ì‚¬í•œ ê¸°ì¡´ í’ˆëª©ì˜ P CODE ì ‘ë‘ë¶€ (ì• 8ìë¦¬)
    closest_code = similar_items[0]['P CODE']
    prefix = closest_code[:8]

    # ì¤‘ë³µ ë°©ì§€
    existing_codes = df_model["P CODE"].dropna().astype(str)
    used_suffixes = [
        int(code[-3:]) for code in existing_codes
        if code.startswith(prefix) and code[-3:].isdigit()
    ]

    next_suffix = max(used_suffixes, default=0) + 1
    return f"{prefix}{str(next_suffix).zfill(3)}"

def generate_structured_pcode(predicted):
    # ì •í™•í•œ í‚¤ ì´ë¦„ìœ¼ë¡œ ì¶”ì¶œ
    l1 = predicted.get("L1 NAME", "")[:2].upper().ljust(2, "X")
    l2 = predicted.get("L2 NAME", "")[:2].upper().ljust(2, "X")
    l3 = predicted.get("L3 NAME", "")[:2].upper().ljust(2, "X")
    l4 = predicted.get("L4 NAME", "")[:2].upper().ljust(2, "X")

    prefix = f"{l1}{l2}{l3}{l4}"

    existing_codes = df_model["P CODE"].dropna().astype(str)
    used_suffixes = [
        int(code[-3:]) for code in existing_codes
        if code.startswith(prefix) and code[-3:].isdigit()
    ]

    next_suffix = max(used_suffixes, default=0) + 1
    new_pcode = f"{prefix}{str(next_suffix).zfill(3)}"

    return new_pcode

def recommend_similar_pcodes_detailed(new_pcode, top_n=5):
    existing = df_model.dropna(subset=["P CODE"])
    existing_codes = existing["P CODE"].astype(str).unique()

    # ë¬¸ì ìœ ì‚¬ë„ ê¸°ë°˜ í›„ë³´ ì¶”ì¶œ
    similar_codes = get_close_matches(new_pcode, existing_codes, n=top_n * 5, cutoff=0.5)  # ë” ë§ì´ ë½‘ê³  í•„í„°ë§

    # ê¸°ì¤€ ë¶„ë¥˜ ì¶”ì¶œ (new_pcode ê¸°ì¤€)
    base_row = {
        "L1": new_pcode[0:2],
        "L2": new_pcode[2:4],
        "L3": new_pcode[4:6],
        "L4": new_pcode[6:8],
    }

    # ìƒì„¸ ì •ë³´ êµ¬ì„± (ë¶„ë¥˜ êµ¬ì¡°ê°€ ë™ì¼í•œ ê²ƒë§Œ)
    results = []
    for code in similar_codes:
        row = existing[existing["P CODE"] == code].iloc[0]
        if (
            row.get("L1 NAME", "")[:2].upper() == base_row["L1"] and
            row.get("L2 NAME", "")[:2].upper() == base_row["L2"] and
            row.get("L3 NAME", "")[:2].upper() == base_row["L3"] and
            row.get("L4 NAME", "")[:2].upper() == base_row["L4"]
        ):
            results.append({
                "P CODE": code,
                "ITEM_NAME": row.get("ITEM_NAME") or row.get("L5 NAME (SPEC)", "N/A"),
                "L1": row.get("L1 NAME", ""),
                "L2": row.get("L2 NAME", ""),
                "L3": row.get("L3 NAME", ""),
                "L4": row.get("L4 NAME", ""),
                "L5": row.get("L5 NAME (SPEC)", "")
            })
        if len(results) >= top_n:
            break

    return pd.DataFrame(results)

# ğŸ”¹ ì—¬ê¸°ê°€ í•¨ìˆ˜ ë°”ê¹¥ì…ë‹ˆë‹¤ (ë§¨ ì™¼ìª½)

def is_valid_combination(predicted: dict, df_model_ref: pd.DataFrame) -> bool:
    l1, l2, l3, l4, l5 = (
        predicted["L1 NAME"],
        predicted["L2 NAME"],
        predicted["L3 NAME"],
        predicted["L4 NAME"],
        predicted["L5 NAME (SPEC)"],
    )
    return not df_model_ref[
        (df_model_ref["L1 NAME"] == l1)
        & (df_model_ref["L2 NAME"] == l2)
        & (df_model_ref["L3 NAME"] == l3)
        & (df_model_ref["L4 NAME"] == l4)
        & (df_model_ref["L5 NAME (SPEC)"] == l5)
    ].empty


def suggest_similar_combination(predicted: dict, df_model_ref: pd.DataFrame, top_n=5):
    query = " ".join([
        predicted.get("L1 NAME", ""),
        predicted.get("L2 NAME", ""),
        predicted.get("L3 NAME", ""),
        predicted.get("L4 NAME", ""),
        predicted.get("L5 NAME (SPEC)", "")
    ])
    input_vec = vectorizer.transform([preprocessor.preprocess_normalized(query)])
    candidate_vecs = vectorizer.transform(df_model_ref["SEARCH_NAME"])
    similarities = cosine_similarity(input_vec, candidate_vecs).flatten()
    top_indices = similarities.argsort()[::-1]
    results = []
    for idx in top_indices:
        row = df_model_ref.iloc[idx]
        results.append({
            "P CODE": row["P CODE"],
            "SEARCH_NAME": row["SEARCH_NAME"],
            "SIMILARITY_SCORE": round(similarities[idx], 3),
        })
        if len(results) >= top_n:
            break
    return pd.DataFrame(results)

def fallback_by_l1_l3(predicted, input_name, top_n=5, similarity_threshold=0.3):
    l1 = predicted.get("L1 NAME", "")
    l2 = predicted.get("L2 NAME", "")
    l3 = predicted.get("L3 NAME", "")

    # í›„ë³´êµ° í•„í„°ë§
    candidates = df_model[
        (df_model["L1 NAME"] == l1) &
        (df_model["L2 NAME"] == l2) &
        (df_model["L3 NAME"] == l3)
    ]

    if candidates.empty:
        return []

    # ì…ë ¥ ë²¡í„° (SEARCH_NAME ê¸°ì¤€ ì „ì²˜ë¦¬)
    input_query = preprocessor.preprocess_normalized(input_name)
    input_vec = vectorizer.transform([input_query])
    candidate_vecs = vectorizer.transform(candidates["SEARCH_NAME"])

    similarities = cosine_similarity(input_vec, candidate_vecs).flatten()
    top_indices = similarities.argsort()[::-1]

    results = []
    for idx in top_indices:
        sim = similarities[idx]
        if sim < similarity_threshold:
            continue  # ì˜ë¯¸ ì—†ëŠ” ì¶”ì²œì€ ì œì™¸

        row = candidates.iloc[idx]
        results.append({
            "SIMILAR_ITEM_NAME": row["SEARCH_NAME"],
            "SIMILARITY_SCORE": round(sim, 3),
            "P CODE": row["P CODE"],
            "L5 NAME (SPEC)": row["L5 NAME (SPEC)"]
        })

        if len(results) >= top_n:
            break

    return results

def recommend_by_similar_l1_l2_l3(input_name, predicted, top_n=5):
    l1, l2, l3 = predicted["L1 NAME"], predicted["L2 NAME"], predicted["L3 NAME"]

    # ë¶„ë¥˜ ì¡°ê±´ì— ë§ëŠ” í›„ë³´ ì¶”ì¶œ
    candidates = df_model[
        (df_model["L1 NAME"] == l1) &
        (df_model["L2 NAME"] == l2) &
        (df_model["L3 NAME"] == l3)
    ]

    if candidates.empty:
        return []

    input_vec = vectorizer.transform([preprocessor.preprocess(input_name)["normalized"]])
    candidate_vecs = vectorizer.transform(candidates["SEARCH_NAME"])
    similarities = cosine_similarity(input_vec, candidate_vecs).flatten()
    top_indices = similarities.argsort()[::-1]

    results = []
    for idx in top_indices:
        row = candidates.iloc[idx]
        results.append({
            "SIMILAR_ITEM_NAME": row["SEARCH_NAME"],
            "SIMILARITY_SCORE": round(similarities[idx], 3),
            "P CODE": row["P CODE"],
            "L5 NAME (SPEC)": row["L5 NAME (SPEC)"]
        })
        if len(results) >= top_n:
            break

    return results

def recommend_global_similar_items(input_name, top_n=5):
    query = preprocessor.preprocess(input_name)["normalized"]
    input_vec = vectorizer.transform([query])
    candidate_vecs = vectorizer.transform(df_model["SEARCH_NAME"])

    similarities = cosine_similarity(input_vec, candidate_vecs).flatten()
    top_indices = similarities.argsort()[::-1]

    results = []
    for idx in top_indices:
        row = df_model.iloc[idx]
        results.append({
            "SIMILAR_ITEM_NAME": row["SEARCH_NAME"],
            "SIMILARITY_SCORE": round(similarities[idx], 3),
            "P CODE": row["P CODE"],
            "L1 NAME": row["L1 NAME"],
            "L2 NAME": row["L2 NAME"],
            "L3 NAME": row["L3 NAME"],
            "L4 NAME": row["L4 NAME"],
            "L5 NAME (SPEC)": row["L5 NAME (SPEC)"],
        })
        if len(results) >= top_n:
            break

    return results

def keyword_fallback_items(input_text, top_n=5):
    words = input_text.upper().split()

    # Step 1: ëª¨ë“  ë‹¨ì–´ í¬í•¨ (AND)
    matches_all = df_model.copy()
    for word in words:
        matches_all = matches_all[matches_all["SEARCH_NAME"].str.contains(word, na=False)]

    if not matches_all.empty:
        return format_keyword_results(matches_all, top_n, mode="AND")

    # Step 2: ì¼ë¶€ ë‹¨ì–´ í¬í•¨ (OR)
    pattern = "|".join(words)
    matches_any = df_model[df_model["SEARCH_NAME"].str.contains(pattern, na=False)]

    if not matches_any.empty:
        return format_keyword_results(matches_any, top_n, mode="OR")

    return []

def format_keyword_results(df_model_matches, top_n=5, mode=""):
    results = []
    for _, row in df_model_matches.head(top_n).iterrows():
        results.append({
            "P CODE": row["P CODE"],
            "SEARCH_NAME": row["SEARCH_NAME"],
            "L1 NAME": row["L1 NAME"],
            "L2 NAME": row["L2 NAME"],
            "L3 NAME": row["L3 NAME"],
            "L4 NAME": row["L4 NAME"],
            "L5 NAME (SPEC)": row["L5 NAME (SPEC)"],
            "MATCH_MODE": mode
        })
    return results

def extract_main_keyword(text, common_words=["ELECTRIC", "PRODUCT", "SET"]):
    words = text.upper().split()
    keywords = [w for w in words if w not in common_words]
    return keywords[0] if keywords else text

# âœ… ëŒ€í™”í˜• ì˜ˆì¸¡ í•¨ìˆ˜
def interactive_product_recommendation():
    user_input = input("ğŸ” í’ˆëª…ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    if not user_input:
        print("â— ì…ë ¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    # 1. ì˜¤íƒ€ ë³´ì •
    corrected = autocorrect_input(user_input)
    if corrected != user_input.upper():
        print(f"ğŸ”§ ì˜¤íƒ€ ë³´ì •: '{user_input}' â†’ '{corrected}'")
    else:
        print(f"âœ… ì…ë ¥ ì¸ì‹ë¨: '{corrected}'")

    # 2. í’ˆëª… ì˜ˆì¸¡
    predicted = predict_item_name(corrected)

    # 3. ì˜ˆì¸¡ ì‹ ë¢°ë„ í‰ê°€ í•¨ìˆ˜
    def is_prediction_reasonable(predicted, user_input, threshold=0.3):
        pred_string = f"{predicted.get('L1 NAME', '')} {predicted.get('L2 NAME', '')} {predicted.get('L3 NAME', '')} {predicted.get('L4 NAME', '')} {predicted.get('L5 NAME (SPEC)', '')}"
        user_vec = vectorizer.transform([user_input.upper()])
        pred_vec = vectorizer.transform([pred_string.upper()])
        sim = cosine_similarity(user_vec, pred_vec)[0][0]
        if len(user_input.strip().split()) <= 1:
            print("âš ï¸ ì…ë ¥ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ì‹ ë¢°ë„ íŒë‹¨ ì—†ì´ fallback ì§„í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return False
        print(f"ğŸ“Š ì˜ˆì¸¡ ì‹ ë¢°ë„ (ì…ë ¥ vs ì˜ˆì¸¡): {round(sim * 100, 1)}%")
        return sim >= threshold

    # 4. ì „ì—­ ìœ ì‚¬ í’ˆëª© ì¶”ì²œ
    global_recommendations = recommend_global_similar_items(corrected)
    if global_recommendations and global_recommendations[0]["SIMILARITY_SCORE"] >= 0.4:
        print("\nğŸ” ì „ì—­ ìœ ì‚¬ í’ˆëª© Top 5:")
        for rec in global_recommendations:
            print(f"[{rec['SIMILARITY_SCORE']}] {rec['SIMILAR_ITEM_NAME']} â†’ {rec['P CODE']}")
    else:
        print("ğŸ“­ ì „ì—­ ìœ ì‚¬ í’ˆëª© ë¶€ì¡±. fallbackì„ ì‹œë„í•©ë‹ˆë‹¤.")

    # 5. ì˜ˆì¸¡ ì‹ ë¢°ë„ ë‚®ìœ¼ë©´ fallback
    if not is_prediction_reasonable(predicted, corrected):
        print("âš ï¸ ì˜ˆì¸¡ëœ ë¶„ë¥˜ê°€ ì…ë ¥ê³¼ ë„ˆë¬´ ë‹¤ë¦…ë‹ˆë‹¤. fallback ì¤‘...")
        similar_items = recommend_similar_items(corrected)

        if similar_items:
            best_fallback = similar_items[0]["SIMILAR_ITEM_NAME"]
            predicted = predict_item_name(best_fallback, verbose=False)
            print(f"ğŸ” fallback ì˜ˆì¸¡ëœ í’ˆëª…: '{best_fallback}' â†’ ì¬ì˜ˆì¸¡ë¨")
        else:
            print("ğŸ“­ ìœ ì‚¬í•œ í’ˆëª…ì´ ì—†ìŒ. L1~L3 ê¸°ë°˜ fallback ì‹œë„ ì¤‘...")
            l3_fallbacks = fallback_by_l1_l3(predicted, corrected)
            if l3_fallbacks:
                best_fallback = l3_fallbacks[0]["SIMILAR_ITEM_NAME"]
                predicted = predict_item_name(best_fallback, verbose=False)
                print(f"ğŸ” L1~L3 ê¸°ë°˜ fallback ì˜ˆì¸¡ëœ í’ˆëª…: '{best_fallback}' â†’ ì¬ì˜ˆì¸¡ë¨")
            else:
                print("âŒ ëª¨ë“  fallback ì‹¤íŒ¨")

    # 6. ìµœì¢… ì˜ˆì¸¡ ë¶„ë¥˜ ì¶œë ¥
    print("\nğŸ¯ ì˜ˆì¸¡ëœ ë¶„ë¥˜:")
    for k, v in predicted.items():
        print(f"- {k}: {v}")

    # 7. ì •í•©ì„± ì²´í¬
    if not is_valid_combination(predicted, df_model):
        print("âŒ ì˜ˆì¸¡ëœ L1~L5 ì¡°í•©ì´ ì‹¤ì œ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("ğŸ” ìœ ì‚¬í•œ L1~L5 ì¡°í•© ì¶”ì²œ:")
        display(suggest_similar_combination(predicted, df_model))

    # 8. ê¸°ì¡´ P CODE í™•ì¸ ë˜ëŠ” ì‹ ê·œ ìƒì„±
    matched = df_model[
        (df_model["L1 NAME"] == predicted["L1 NAME"]) &
        (df_model["L2 NAME"] == predicted["L2 NAME"]) &
        (df_model["L3 NAME"] == predicted["L3 NAME"]) &
        (df_model["L4 NAME"] == predicted["L4 NAME"]) &
        (df_model["L5 NAME (SPEC)"] == predicted["L5 NAME (SPEC)"])
    ]

    if not matched.empty:
        existing_pcode = matched["P CODE"].iloc[0]
        print(f"- ğŸ“¦ ì¶”ì²œ P CODE (ê¸°ì¡´): {existing_pcode}")
    else:
        # global_recommendations ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ similar_items ëŒ€ì²´
        base_items = global_recommendations or recommend_similar_items(corrected)
        new_pcode = generate_structured_pcode_based_on_similar(base_items, predicted)
        print(f"- ğŸ†• ì¶”ì²œ P CODE (ì‹ ê·œ): {new_pcode}")
        similar_details_df_model = recommend_similar_pcodes_detailed(new_pcode)
        if not similar_details_df_model.empty:
            print("\nğŸ“‹ ìœ ì‚¬í•œ ê¸°ì¡´ P CODE ë° í’ˆëª…:")
            display(similar_details_df_model)
        else:
            print("ğŸ“­ ìœ ì‚¬í•œ ê¸°ì¡´ P CODE ì—†ìŒ")

    # 9. TF-Idf_model ìœ ì‚¬ í’ˆëª© ì¶”ì²œ
    recommendations = recommend_similar_items(corrected)
    if recommendations and any(rec['SIMILARITY_SCORE'] > 0 for rec in recommendations):
        print("\nğŸ” ìœ ì‚¬í•œ í’ˆëª© Top 5:")
        for rec in recommendations:
            print(f"[{rec['SIMILARITY_SCORE']}] {rec['SIMILAR_ITEM_NAME']} â†’ {rec['P CODE']}")
    else:
        print("ğŸ“­ TF-Idf_model ê¸°ë°˜ ì¶”ì²œ ì‹¤íŒ¨. ì „ì—­ ê²€ìƒ‰ ì‹œë„ ì¤‘...")
        recommendations = global_recommendations or []
        if not recommendations:
            print("ğŸ“­ ì „ì—­ ìœ ì‚¬ í’ˆëª©ë„ ì—†ìŒ. í‚¤ì›Œë“œ í¬í•¨ ê²€ìƒ‰ ì‹œë„ ì¤‘...")
            keyword_matches = keyword_fallback_items(corrected)
            if keyword_matches:
                print("\nğŸ” ë‹¨ì–´ í¬í•¨ í’ˆëª© ì¶”ì²œ:")
                for item in keyword_matches:
                    print(f"ğŸ”¹ [{item.get('MATCH_MODE', 'í‚¤ì›Œë“œ')}] {item['SEARCH_NAME']} â†’ {item['P CODE']}")
            else:
                print("âŒ ë‹¨ì–´ í¬í•¨ ê²°ê³¼ë„ ì—†ìŒ.")


# ai_model_module.py ë‚´ë¶€ì— ì¶”ê°€í•  ì½”ë“œ
def set_globals(vec, df_model_param, prep, models, encoders, valid_combos=None):
    global vectorizer, df_model, preprocessor
    global hierarchical_models, hierarchical_label_encoders, valid_combinations

    vectorizer = vec
    df_model = df_model_param
    preprocessor = prep
    hierarchical_models = models
    hierarchical_label_encoders = encoders
    valid_combinations = valid_combos or set()

